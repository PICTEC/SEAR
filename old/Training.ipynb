{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DATASETPATH = \"/home/zantyr/.dataset\"\n",
    "MODEL_ROOT = \"./models\"\n",
    "\n",
    "train_source = np.load(os.path.join(DATASETPATH, \"train_source.bin.npy\"))\n",
    "valid_source = np.load(os.path.join(DATASETPATH, \"valid_source.bin.npy\"))\n",
    "train_target = np.load(os.path.join(DATASETPATH, \"train_target.bin.npy\"))\n",
    "valid_target = np.load(os.path.join(DATASETPATH, \"valid_target.bin.npy\"))\n",
    "\n",
    "# set this to \"-1\" to disable GPU, set this to other numbers in multi-GPU systems\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/tensorflow/python/ops/distributions/distribution.py:265: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/tensorflow/python/ops/distributions/bernoulli.py:169: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1248, 129)         0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1248, 129)         0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 1248, 129, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1248, 129, 8)      48        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 1248, 129, 8)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1248, 129, 8)      32        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1248, 129, 8)      328       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1248, 129, 8)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1248, 129, 8)      32        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1248, 129, 8)      328       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1248, 129, 8)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1248, 129, 8)      32        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1248, 129, 8)      328       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1248, 129, 8)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1248, 129, 8)      32        \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1248, 129, 8)      328       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1248, 129, 8)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1248, 129, 8)      32        \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1248, 129, 8)      328       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 1248, 129, 8)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1248, 129, 8)      32        \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 1248, 8, 129)      0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1248, 8, 96)       12480     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 1248, 8, 96)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1248, 8, 96)       384       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1248, 8, 64)       6208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 1248, 8, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1248, 8, 64)       256       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1248, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1248, 257)         131584    \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 1248, 257)         0         \n",
      "=================================================================\n",
      "Total params: 152,792\n",
      "Trainable params: 152,376\n",
      "Non-trainable params: 416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Lambda, LeakyReLU, TimeDistributed, Flatten, BatchNormalization\n",
    "import keras.backend as K\n",
    "\n",
    "MAX = train_source.max()\n",
    "\n",
    "def mk_model():\n",
    "    wrap = lambda x: BatchNormalization()(LeakyReLU(0.01)(x))\n",
    "    nw_input = lyr = Input((1248, 129))\n",
    "    lyr = Lambda(lambda x: x / MAX)(lyr)\n",
    "    lyr = Lambda(K.expand_dims)(lyr)\n",
    "    lyr = wrap(Conv2D(8, [5,1], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [1,5], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [5,1], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [1,5], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [5,1], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [1,5], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = Lambda(lambda x:K.permute_dimensions(x, (0,1,3,2)))(lyr)\n",
    "    lyr = wrap(Dense(96)(lyr))\n",
    "    lyr = wrap(Dense(64)(lyr))\n",
    "    lyr = TimeDistributed(Flatten())(lyr)\n",
    "    lyr = Dense(257, use_bias=False)(lyr)\n",
    "    lyr = Lambda(lambda x: x * MAX)(lyr)\n",
    "    model = Model(nw_input, lyr)\n",
    "    return model\n",
    "\n",
    "import keras.callbacks as kc\n",
    "\n",
    "mk_model().summary()\n",
    "\n",
    "def training(model):\n",
    "    for lr in [3e-4, 1e-4, 3e-5, 1e-5, 3e-6, 1e-6]:\n",
    "        model.compile(keras.optimizers.Adam(lr), 'mse')\n",
    "        model.fit(train_source, train_target, validation_data=(valid_source, valid_target), batch_size=8, epochs=100, callbacks=[kc.TensorBoard(\"./tf_logs\"), kc.EarlyStopping()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 4s 24ms/step - loss: 1058.8642 - val_loss: 933.0909\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 581.8323 - val_loss: 775.0587\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 280.9676 - val_loss: 734.1549\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 165.9070 - val_loss: 581.3661\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 101.4906 - val_loss: 369.1296\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 70.7532 - val_loss: 242.3070\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 56.9242 - val_loss: 170.1039\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 46.0996 - val_loss: 132.2921\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 41.5238 - val_loss: 92.7083\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 36.8887 - val_loss: 79.9211\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 30.7126 - val_loss: 62.4137\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 31.3742 - val_loss: 54.2988\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 29.4104 - val_loss: 43.3365\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 25.4562 - val_loss: 35.2151\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 28.5421 - val_loss: 32.2792\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 22.3268 - val_loss: 30.6284\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 23.2429 - val_loss: 26.7246\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 20.1738 - val_loss: 25.8700\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 21.8861 - val_loss: 23.1521\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 20.1839 - val_loss: 21.3786\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 18.1783 - val_loss: 21.5494\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 19.5050 - val_loss: 20.9956\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 17.7204 - val_loss: 21.5869\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 17.6544 - val_loss: 19.2083\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 16.7372 - val_loss: 18.5665\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 16.3874 - val_loss: 17.2786\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 16.3288 - val_loss: 16.8808\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 16.0643 - val_loss: 16.7772\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 16.2567 - val_loss: 16.0551\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 15.7191 - val_loss: 15.8351\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 16.5160 - val_loss: 15.6886\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 15.5574 - val_loss: 15.5558\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 14.5569 - val_loss: 15.0424\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 15.2500 - val_loss: 14.8643\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 15.3472 - val_loss: 14.9385\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 14.0766 - val_loss: 14.6018\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.6485 - val_loss: 14.3518\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.9302 - val_loss: 14.1733\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 15.5282 - val_loss: 14.1277\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.4966 - val_loss: 14.0733\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 15.5109 - val_loss: 14.0133\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.2906 - val_loss: 13.8949\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.5380 - val_loss: 13.8415\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.8987 - val_loss: 13.9040\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 14.3203 - val_loss: 13.7658\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 15.1453 - val_loss: 13.7354\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.6617 - val_loss: 13.7113\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.3371 - val_loss: 13.6583\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 15.2020 - val_loss: 13.6614\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 14.8720 - val_loss: 13.6440\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.8660 - val_loss: 13.6423\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.7883 - val_loss: 13.6414\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 15.0357 - val_loss: 13.6380\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 14.9891 - val_loss: 13.6425\n",
      "Model path:  ./models/2019-04-05T14:44:45.154857.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f16e8664160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def experiment(mk_model):\n",
    "    model = mk_model()\n",
    "    name = datetime.datetime.now().isoformat() + \".h5\"\n",
    "    try:\n",
    "        training(model)\n",
    "    finally:\n",
    "        keras.models.save_model(model, os.path.join(MODEL_ROOT, name))\n",
    "        print(\"Model path: \", os.path.join(MODEL_ROOT, name))\n",
    "    return model\n",
    "        \n",
    "experiment(mk_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Lambda, LeakyReLU, TimeDistributed, Flatten, BatchNormalization\n",
    "import keras.backend as K\n",
    "\n",
    "MAX = train_source.max()\n",
    "\n",
    "def mk_model():\n",
    "    wrap = lambda x: BatchNormalization()(LeakyReLU(0.01)(x))\n",
    "    nw_input = lyr = Input((1248, 129))\n",
    "    lyr = Lambda(lambda x: x / MAX)(lyr)\n",
    "    lyr = Lambda(K.expand_dims)(lyr)\n",
    "    lyr = wrap(Conv2D(8, [5,1], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [1,5], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [5,1], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [1,5], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [5,1], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = wrap(Conv2D(8, [1,5], padding=\"same\", activation=None)(lyr))\n",
    "    lyr = Lambda(lambda x:K.permute_dimensions(x, (0,1,3,2)))(lyr)\n",
    "    lyr = wrap(Dense(96)(lyr))\n",
    "    lyr = wrap(Dense(64)(lyr))\n",
    "    lyr = TimeDistributed(Flatten())(lyr)\n",
    "    lyr = Dense(257, use_bias=False)(lyr)\n",
    "    lyr = Lambda(lambda x: x * MAX)(lyr)\n",
    "    model = Model(nw_input, lyr)\n",
    "    return model\n",
    "\n",
    "import keras.callbacks as kc\n",
    "\n",
    "mk_model().summary()\n",
    "\n",
    "def training(model):\n",
    "    for lr in [3e-4, 1e-4, 3e-5, 1e-5, 3e-6, 1e-6]:\n",
    "        model.compile(keras.optimizers.Adam(lr), 'mse')\n",
    "        model.fit(train_source, train_target, validation_data=(valid_source, valid_target), batch_size=8, epochs=100, callbacks=[kc.TensorBoard(\"./tf_logs\"), kc.EarlyStopping()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.937874"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
